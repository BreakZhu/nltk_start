# -*- coding: utf-8 -*-
import math
import nltk

# 6.4 决策树
"""
决策树、朴素贝叶斯分类器和最大熵分类器。正如我们所看到的，可以把这些学习方法看作黑盒
子，直接训练模式，使用它们进行预测而不需要理解它们是如何工作的。但是，仔细看看这
些学习方法如何基于一个训练集上的数据选择模型，会学到很多。了解这些方法可以帮助指
导我们选择相应的特征，尤其是我们关于那些特征如何编码的决定。理解生成的模型可以让
我们更好的提取信息，哪些特征对有信息量，那些特征之间如何相互关联
"""
"""
决策树是一个简单的为输入值选择标签的流程图。这个流程图由检查特征值的决策节点
和分配标签的叶节点组成。为输入值选择标签，我们以流程图的初始决策节点（称为其根节
点）开始。此节点包含一个条件，检查输入值的特征之一，基于该特征的值选择一个分支。
沿着这个描述我们输入值的分支，我们到达了一个新的决策节点，有一个关于输入值的特征
的新的条件。我们继续沿着每个节点的条件选择的分支，直到到达叶节点，它为输入值提供
了一个标签
"""

# 熵和信息增益
"""
决策树桩确定最有信息量的特征。一种流行的替代方
法，被称为信息增益，当我们用给定的特征分割输入值时，衡量它们变得更有序的程度。要
衡量原始输入值集合如何无序，我们计算它们的标签的墒，如果输入值的标签非常不同，墒
就高；如果输入值的标签都相同，墒就低。特别是，熵被定义为每个标签的概率乘以那个标
签的 log 概率的总和。
(1) H=Σl∈labelsP(l) × log2P(l)
"""


# 例 6-8. 计算标签链表的墒
def entropy(labels):
    freqdist = nltk.FreqDist(labels)
    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]
    return -sum([p * math.log(p, 2) for p in probs])

print entropy(['male', 'male', 'male', 'male'])
print entropy(['male', 'female', 'male', 'male'])
print entropy(['female', 'male', 'female', 'male'])
print entropy(['female', 'female', 'male', 'female'])
print entropy(['female', 'female', 'female', 'female'])
"""
一旦我们已经计算了原始输入值的标签集的墒，就可以判断应用了决策树桩之后标签会变得多么有序。为了这样做，
我们计算每个决策树桩的叶子的熵，利用这些叶子熵值的平均值（加权每片叶子的样本数量）。
信息增益等于原来的熵减去这个新的减少的熵。信息增益越高，将输入值分为相关组的决策树桩就越好，
于是我们可以通过选择具有最高信息增益的决策树桩来建立决策树.
决策树的另一个考虑因素是效率。前面描述的选择决策树桩的简单算法必须为每一个可
能的特征构建候选决策树桩，并且这个过程必须在构造决策树的每个节点上不断重复。已经
开发了一些算法通过存储和重用先前评估的例子的信息减少训练时间
决策树特别适合有很多层次的分类区别的情况。例如：决策树可以非常有效地捕捉进化树。

决策树也有一些缺点。一个问题是，由于决策树的每个分支会划分训练数据，在
训练树的低节点，可用的训练数据量可能会变得非常小。因此，这些较低的决策节点可能过
拟合训练集，学习模式反映训练集的特质而不是问题底层显著的语言学模式。对这个问题的
一个解决方案是当训练数据量变得太小时停止分裂节点。
另一种方案是长出一个完整的决策树，但随后进行剪枝剪去在开发测试集上不能提高性能的决策节点
预剪枝 后剪枝  限制数的高度
"""
