文本 学习分类文本
有监督分类
    性别鉴定
def gender_features(word):
    return {'last_letter': word[-1]}
gender_features('Shrek')

from nltk.corpus import names
import random
names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])
random.shuffle(names)
选择正确的特征
特征提取通过反复试验和错误的过程建立的，由哪些信息是与问题相关的直觉指引的。它通常以“厨房水槽”的方法开始，
包括你能想到的所有特征，然后检查哪些特征是实际有用的
例 6-1. 一个特征提取器，过拟合性别特征。这个特征提取器返回的特征集包括大量指定的特征，从而导致对于相对较小的名字语料库过拟合
def gender_features2(name):
    features = {}
    features["firstletter"] = name[0].lower()
    features["lastletter"] = name[–1].lower()
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        features["count(%s)" % letter] = name.lower().count(letter)
        features["has(%s)" % letter] = (letter in name.lower())
    return features
gender_features2('John')

def gender_features3(word):
    return {'suffix1': word[-1:], 'suffix2': word[-2:], 'perfix_1': word[:1], "perfix_2": word[:2]}

"""
你要用于一个给定的学习算法的特征的数目是有限的——如果你提供太多的特
征，那么该算法将高度依赖你的训练数据的特，性而一般化到新的例子的效果不会很好。这
个问题被称为过拟合
"""
featuresets = [(gender_features2(n), g) for (n, g) in names]
train_set, test_set = featuresets[500:], featuresets[:500]
classifier = nltk.NaiveBayesClassifier.train(train_set)
print nltk.classify.accuracy(classifier, test_set)

"""
yn 结尾的名字显示以女性为主，尽管事实上，n 结尾的名字往往是男性；以 ch 结尾的名字通常
是男性，尽管以 h 结尾的名字倾向于是女性
"""
train_names = names[1500:]
devtest_names = names[500:1500]
test_names = names[:500]
train_set = [(gender_features3(n), g) for (n, g) in train_names]  # 训练集数据
devtest_set = [(gender_features3(n), g) for (n, g) in devtest_names]  # 开发测试集
test_set = [(gender_features3(n), g) for (n, g) in test_names]  # 测试
classifier = nltk.NaiveBayesClassifier.train(train_set)
print nltk.classify.accuracy(classifier, devtest_set)
errors = []
for (name, tag) in devtest_names:
    guess = classifier.classify(gender_features3(name))
    if guess != tag:
        errors.append((tag, guess, name))
for (tag, guess, name) in sorted(errors):  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
    print 'correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name)

"""我们为文档定义一个特征提取器，这样分类器就会知道哪些方面的数据应注意
（见例 6-2）。对于文档主题识别，我们可以为每个词定义一个特性表示该文档是否包含这
个词。为了限制分类器需要处理的特征的数目，我们一开始构建一个整个语料库中前 2000
个最频繁词的链表。然后，定义一个特征提取器，简单地检查这些词是否在一个给定的文档中。
例 6-2. 一个文档分类的特征提取器，其特征表示每个词是否在一个给定的文档中
"""
all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())  # 获取影评中所有数据
word_featrues = all_words.keys()[:2000]


def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_featrues:
        features['contains(%s)' % word] = (word in document_words)
    return features
print document_features(movie_reviews.words('pos/cv957_8737.txt'))


"""
训练和测试一个分类器进行文档分类
"""
featrueset = [(document_features(d), c) for (d, c) in documents]
train_set, test_set = featrueset[200:], featrueset[:200]
classifier = nltk.NaiveBayesClassifier.train(train_set)
print nltk.classify.accuracy(classifier, train_set)
classifier.show_most_informative_features(5)
"""
Most Informative Features
          contains(sans) = True              neg : pos    =      9.0 : 1.0
    contains(mediocrity) = True              neg : pos    =      7.7 : 1.0
   contains(bruckheimer) = True              neg : pos    =      6.4 : 1.0
         contains(wires) = True              neg : pos    =      6.4 : 1.0
         contains(tripe) = True              neg : pos    =      6.4 : 1.0
提到 Seagal 的评论中负面的是正面的大约 8 倍，而提到 Damon 的评论中正面的是负面的大约 6 倍
"""
第 5 章中，我们建立了一个正则表达式标注器，通过查找词内部的组成，为词选择词性
标记。然而，这个正则表达式标注器是手工制作的。作为替代，我们可以训练一个分类器来
算出哪个后缀最有信息量。首先，让我们找出最常见的后缀

suffix_fdist = nltk.FreqDist()
for word in brown.words():
    word = word.lower()
    suffix_fdist[word[-1:]] += 1
    suffix_fdist[word[-2:]] += 1
    suffix_fdist[word[-3:]] += 1
common_suffixes = suffix_fdist.keys()[:100]
print common_suffixes


def pos_features(word):
    """
    定义一个特征提取器函数，检查给定的单词的这些后缀
    :param word:
    :return:
    """
    features = {}
    for suffix in common_suffixes:
        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)
    return features

"""
特征提取函数的行为就像有色眼镜一样，强调我们的数据中的某些属性（颜色），并使
其无法看到其他属性。分类器在决定如何标记输入时，将完全依赖它们强调的属性。在这种
情况下，分类器将只基于一个给定的词拥有（如果有）哪个常见后缀的信息来做决定。
现在，我们已经定义了我们的特征提取器，可以用它来训练一个新的“决策树”的分类
器
"""
# 使用决策树
tagged_word = brown.tagged_words(categories="news")
featuresets = [(pos_features(n), g) for (n, g) in tagged_word]
size = int(len(featuresets) * 0.1)
train_set, test_set = featuresets[size:], featuresets[:size]
classifier = nltk.DecisionTreeClassifier.train(train_set)
print nltk.classify.accuracy(classifier, test_set)
print classifier.classify(pos_features('cats'))
# 输出树的层级
print classifier.pseudocode(depth=4)
 endswith(,) == True: return ','
if endswith(,) == False:
if endswith(the) == True: return 'AT'
if endswith(the) == False:
if endswith(s) == True:
if endswith(is) == True: return 'BEZ'
if endswith(is) == False: return 'VBZ'
if endswith(s) == False:
if endswith(.) == True: return '.'
if endswith(.) == False: return 'NN'
在这里，我们可以看到分类器一开始检查一个词是否以逗号结尾——如果是，它会得到
一个特别的标记“,”。接下来，分类器检查词是否以“the”结尾，这种情况它几乎肯定是
一个限定词。这个“后缀”被决策树早早使用是因为词 the 太常见。分类器继续检查词是否
以 s 结尾，如果是，那么它极有可能得到动词标记 VBZ（除非它是这个词 is，它有特殊标
记 BEZ），如果不是，那么它往往是名词（除非它是标点符号“.”）。实际的分类器包含这里
显示的 if-then 语句下面进一步的嵌套，参数 depth=4 只显示决定树的顶端部分。

"""
为了捕捉相关的分类任务之间的依赖关系，我们可以使用联合分类器模型，收集有关输
入，选择适当的标签。在词性标注的例子中，各种不同的序列分类器模型可以被用来为一个
给定的句子中的所有的词共同选择词性标签。
一种序列分类器策略，称为连续分类或贪婪序列分类，是为第一个输入找到最有可能的
类标签，然后使用这个问题的答案帮助找到下一个输入的最佳的标签。这个过程可以不断重
复直到所有的输入都被贴上标签。这是被 5.5 节的 bigram 标注器采用的方法，它一开始为
句子的第一个词选择词性标记，然后为每个随后的词选择标记，基于词本身和前面词的预测
的标记。
在例 6-5 演示了这一策略。首先，我们必须扩展我们的特征提取函数使其具有参数 his
tory，它提供一个我们到目前为止已经为句子预测的标记的链表�。history 中的每个标记
对应句子中的一个词。但是请注意，history 将只包含我们已经归类的词的标记，也就是目
标词左侧的词。因此，虽然是有可能查看目标词右边的词的某些特征，但查看那些词的标记
是不可能的（因为我们还未产生它们）。
已经定义了特征提取器，我们可以继续建立我们的序列分类器。在训练中，我们使用
已标注的标记为征提取器提供适当的历史信息，但标注新的句子时，我们基于标注器本身的
输出产生历史信息。
例 6-5. 使用连续分类器进行词性标注。
"""
def pos_features(sentence, i, history):
    features = {"suffix(1)": sentence[i][-1:],
                "suffix(2)": sentence[i][-2:],
                "suffix(3)": sentence[i][-3:]}
    if i == 0:
        features["prev-word"] = "<START>"
        features["prev-tag"] = "<START>"
    else:
        features["prev-word"] = sentence[i - 1]
        features["prev-tag"] = history[i - 1]
    return features


class ConsecutivePosTagger(nltk.TaggerI):
    def __init__(self, train_sents):
        train_set = []
        for tagged_sent in train_sents:
            untagged_sent = nltk.tag.untag(tagged_sent)
            history = []
            for i, (word, tag) in enumerate(tagged_sent):
                featureset = pos_features(untagged_sent, i, history)
                train_set.append((featureset, tag))
                history.append(tag)
        self.classifier = nltk.NaiveBayesClassifier.train(train_set)

    def tag(self, sentence):
        history = []
        for i, word in enumerate(sentence):
            featureset = pos_features(sentence, i, history)
            tag = self.classifier.classify(featureset)
            history.append(tag)
        return zip(sentence, history)

tagged_sents = brown.tagged_sents(categories='news')
size = int(len(tagged_sents) * 0.1)
train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]
tagger = ConsecutivePosTagger(train_sents)
print tagger.evaluate(test_sents)

# 句子分割可以看作是一个标点符号的分类任务：每当我们遇到一个可能会结束一个句子
# 的符号，如句号或问号，我们必须决定它是否终止了当前句子。
# 第一步是获得一些已被分割成句子的数据，将它转换成一种适合提取特征的形式

sents = nltk.corpus.treebank_raw.sents()
tokens = []
boundaries = set()
offset = 0

# tokens 是单独句子标识符的合并链表，boundaries 是一个包含所有句子边界标识符索引的集合
for sent in nltk.corpus.treebank_raw.sents():
    tokens.extend(sent)  # 函数用于在列表末尾一次性追加另一个序列中的多个值
    offset += len(sent)
    boundaries.add(offset - 1)  # 每次存储句子位置


def punct_features(tokens, i):
    """
    我们需要指定用于决定标点是否表示句子边界的数据特征
    :param tokens:
    :param i:               判断下一句开始是否为大写，前一句结束为小写， 前一个词是否为单字母
    :return:
    """
    return {'next-word-capitalized': tokens[i + 1][0].isupper(), 'prevword': tokens[i - 1].lower(), 'punct': tokens[i],
            'prev-word-is-one-char': len(tokens[i - 1]) == 1}


# 基于这一特征提取器，我们可以通过选择所有的标点符号创建一个加标签的特征集的链表，然后标注它们是否是边界标识符
# 通过判断是否是 boundaries 确定是否是边界标识符

featuresets = [(punct_features(tokens, i), (i in boundaries)) for i in range(1, len(tokens) - 1) if tokens[i] in '.?!']
size = int(len(featuresets) * 0.1)
train_set, test_set = featuresets[size:], featuresets[:size]
classifier = nltk.NaiveBayesClassifier.train(train_set)
print nltk.classify.accuracy(classifier, test_set)
print classifier.labels()


# 例 6-6. 基于分类的断句器

def segment_sentences(words):
    start = 0
    sents = []
    for i, word in words:
        if word in '.?!' and classifier.classify(words, i) == True:
            sents.append(words[start:i + 1])
            start = i + 1
    if start < len(words):
        sents.append(words[start:])


# 识别对话行为类型
"""
NPS 聊天语料库
包括超过 10,000 个来自即时消息会话的帖子。这些帖子都已经被贴上 15 种对话行为类型中的一种标签，
例如：“陈述”，“情感”，“yn 问
题”，“Continuer”。因此，我们可以利用这些数据建立一个分类器，识别新的即时消息帖子
的对话行为类型。第一步是提取基本的消息数据。我们将调用 xml_posts()来得到一个数
据结构，表示每个帖子的 XML 注释
"""
posts = nltk.corpus.nps_chat.xml_posts()[:10000]


# 下一步，我们将定义一个简单的特征提取器，检查帖子包含什么词：
def dialogue_act_features(post):
    features = {}
    for word in nltk.word_tokenize(post):
        features['contains(%s)' % word.lower()] = True
    return features


"""
通过为每个帖子提取特征（使用 post.get('class') 获得一个帖子的对话行
为类型）构造训练和测试数据，并创建一个新的分类器
"""
featuresets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]
size = int(len(featuresets) * 0.1)
train_set, test_set = featuresets[size:], featuresets[:size]
classifier = nltk.NaiveBayesClassifier.train(train_set)
print nltk.classify.accuracy(classifier, test_set)

# 识别文字蕴含
"""
应当强调，文字和假设之间的关系并不一定是逻辑蕴涵，而是一个人是否会得出结论：文本提供了合理的证据证明假设是真实的。
我们可以把 RTE 当作一个分类任务，尝试为每一对预测真/假标签。虽然这项任务的成功做法似乎看上去涉及语法分析、
语义和现实世界的知识的组合，RTE 的许多早期的尝试使用粗浅的分析基于文字和假设之间的在词级别的相似性取得了相当不错的结果。
在理想情况下，我们希望如果有一个蕴涵那么假设所表示的所有信息也应该在文本中表示。相反，如果假设中有的资料文本中没有，
那么就没有蕴涵。在我们的 RTE 特征探测器（例 6-7）中，我们让词（即词类型）作为信息的代理，我们的特征计数词重叠的程度和
假设中有而文本中没有的词的程度（由 hyp_extra()方法获取）。不是所有的词都是同样重要的——命名实体，
如人、组织和地方的名称，可能会更为重要，这促使我们分别为 words 和 nes（命名实体）提取不同的信息。
此外，一些高频虚词作为“停用词”被过滤掉。
图 6-7. “认识文字蕴涵”的特征提取器。RTEFeatureExtractor 类建立了一个除去一些停用词后在文本和假设中都有的词汇包，
然后计算重叠和差异
"""


def rte_features(rtepair):
    extractor = nltk.RTEFeatureExtractor(rtepair)   # 词的覆盖                                  # 假设中有儿实际没有
    features = {'word_overlap': len(extractor.overlap('word')), 'word_hyp_extra': len(extractor.hyp_extra('word')),
                'ne_overlap': len(extractor.overlap('ne')), 'ne_hyp_extra': len(extractor.hyp_extra('ne'))}
    return features
# 为了说明这些特征的内容，我们检查前面显示的文本/假设对 34 的一些属性
rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]
extractor = nltk.RTEFeatureExtractor(rtepair)
print extractor.text_words
print extractor.hyp_words
print extractor.overlap('word')
print extractor.overlap('ne')
print extractor.hyp_extra('word')
# 6.3 评估

# 测试集
import random
from nltk.corpus import brown

tagged_sents = list(brown.tagged_sents(categories='news'))
random.shuffle(tagged_sents)
size = int(len(tagged_sents) * 0.1)
train_set, test_set = tagged_sents[size:], tagged_sents[:size]
"""
在这种情况下，我们的测试集和训练集将是非常相似的。训练集和测试集均取自同一文
体，所以我们不能相信评估结果可以推广到其他文体。更糟糕的是，因为调用 random.shuffle()，
测试集中包含来自训练使用过的相同的文档的句子。如果文档中有相容的模式（也
就是说，如果一个给定的词与特定词性标记一起出现特别频繁），那么这种差异将体现在开
发集和测试集。一个稍好的做法是确保训练集和测试集来自不同的文件
"""
file_ids = brown.fileids(categories='news')
size = int(len(file_ids) * 0.1)
# train_set = brown.tagged_sents(file_ids[size:])
# test_set = brown.tagged_sents(file_ids[:size])
train_set = brown.tagged_sents(categories='news')
test_set = brown.tagged_sents(categories='fiction')

# 准确度
"""
用于评估一个分类最简单的度量是准确度，测量测试集上分类器正确标注的输入的比
例。例如：一个名字性别分类器，在包含 80 个名字的测试集上预测正确的名字有 60 个，它
有 60/80= 75％的准确度。nltk.classify.accuracy()函数会在给定的测试集上计算分类器
模型的准确度
"""
# classifier = nltk.NaiveBayesClassifier.train(train_set)
# print 'Accuracy: %4.2f' % nltk.classify.accuracy(classifier, test_set)

# 精确度和召回率
"""
• 真阳性是相关项目中我们正确识别为相关的。
• 真阴性是不相关项目中我们正确识别为不相关的。
• 假阳性（或 I 型错误）是不相关项目中我们错误识别为相关的。
• 假阴性（或 II 型错误）是相关项目中我们错误识别为不相关的
"""
"""
精确度（Precision），表示我们发现的项目中有多少是相关的，TP/(TP+ FP)。
召回率（Recall），表示相关的项目中我们发现了多少，TP/(TP+ FN)。
F-度量值（F-Measure）（或 F-得分，F-Score），组合精确度和召回率为一个单独的得分，
被定义为精确度和召回率的调和平均数(2 × Precision × Recall)/(Precision+Recall)
"""

# 混淆矩阵
"""
当处理有 3 个或更多的标签的分类任务时，基于模型错误类型细分模型的错误是有信息
量的。一个混淆矩阵是一个表，其中每个 cells[i,j]表示正确的标签 i 被预测为标签 j 的次数。
因此，对角线项目（即 cells[i,i]）表示正确预测的标签，非对角线项目表示错误。在下面的
例子中，我们为 5.4 节中开发的 unigram 标注器生成一个混淆矩阵
"""

# 交叉验证
"""
为了评估我们的模型，我们必须为测试集保留一部分已标注的数据。正如我们已经提到，
如果测试集是太小了，我们的评价可能不准确。然而，测试集设置较大通常意味着训练集设
置较小，如果已标注数据的数量有限，这样设置对性能会产生重大影响。
这个问题的解决方案之一是在不同的测试集上执行多个评估，然后组合这些评估的得
分，这种技术被称为交叉验证。特别是，我们将原始语料细分为 N 个子集称为折叠（folds）。
对于每一个这些的折叠，我们使用除这个折叠中的数据外其他所有数据训练模型，然后在这
个折叠上测试模型。即使个别的折叠可能是太小了而不能在其上给出准确的评价分数，综合
评估得分是基于大量的数据，因此是相当可靠的。
第二，同样重要的，采用交叉验证的优势是，它可以让我们研究不同的训练集上性能变
化有多大。如果我们从所有 N 个训练集得到非常相似的分数，然后我们可以相当有信心，
得分是准确的。另一方面，如果 N 个训练集上分数很大不同，那么，我们应该对评估得分
的准确性持怀疑态度
"""
# 6.4 决策树
"""
决策树、朴素贝叶斯分类器和最大熵分类器。正如我们所看到的，可以把这些学习方法看作黑盒
子，直接训练模式，使用它们进行预测而不需要理解它们是如何工作的。但是，仔细看看这
些学习方法如何基于一个训练集上的数据选择模型，会学到很多。了解这些方法可以帮助指
导我们选择相应的特征，尤其是我们关于那些特征如何编码的决定。理解生成的模型可以让
我们更好的提取信息，哪些特征对有信息量，那些特征之间如何相互关联
"""
"""
决策树是一个简单的为输入值选择标签的流程图。这个流程图由检查特征值的决策节点
和分配标签的叶节点组成。为输入值选择标签，我们以流程图的初始决策节点（称为其根节
点）开始。此节点包含一个条件，检查输入值的特征之一，基于该特征的值选择一个分支。
沿着这个描述我们输入值的分支，我们到达了一个新的决策节点，有一个关于输入值的特征
的新的条件。我们继续沿着每个节点的条件选择的分支，直到到达叶节点，它为输入值提供
了一个标签
"""

# 熵和信息增益
"""
决策树桩确定最有信息量的特征。一种流行的替代方
法，被称为信息增益，当我们用给定的特征分割输入值时，衡量它们变得更有序的程度。要
衡量原始输入值集合如何无序，我们计算它们的标签的墒，如果输入值的标签非常不同，墒
就高；如果输入值的标签都相同，墒就低。特别是，熵被定义为每个标签的概率乘以那个标
签的 log 概率的总和。
(1) H=Σl∈labelsP(l) × log2P(l)
"""


# 例 6-8. 计算标签链表的墒
def entropy(labels):
    freqdist = nltk.FreqDist(labels)
    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]
    return -sum([p * math.log(p, 2) for p in probs])

print entropy(['male', 'male', 'male', 'male'])
print entropy(['male', 'female', 'male', 'male'])
print entropy(['female', 'male', 'female', 'male'])
print entropy(['female', 'female', 'male', 'female'])
print entropy(['female', 'female', 'female', 'female'])
"""
一旦我们已经计算了原始输入值的标签集的墒，就可以判断应用了决策树桩之后标签会变得多么有序。为了这样做，
我们计算每个决策树桩的叶子的熵，利用这些叶子熵值的平均值（加权每片叶子的样本数量）。
信息增益等于原来的熵减去这个新的减少的熵。信息增益越高，将输入值分为相关组的决策树桩就越好，
于是我们可以通过选择具有最高信息增益的决策树桩来建立决策树.
决策树的另一个考虑因素是效率。前面描述的选择决策树桩的简单算法必须为每一个可
能的特征构建候选决策树桩，并且这个过程必须在构造决策树的每个节点上不断重复。已经
开发了一些算法通过存储和重用先前评估的例子的信息减少训练时间
决策树特别适合有很多层次的分类区别的情况。例如：决策树可以非常有效地捕捉进化树。

决策树也有一些缺点。一个问题是，由于决策树的每个分支会划分训练数据，在
训练树的低节点，可用的训练数据量可能会变得非常小。因此，这些较低的决策节点可能过
拟合训练集，学习模式反映训练集的特质而不是问题底层显著的语言学模式。对这个问题的
一个解决方案是当训练数据量变得太小时停止分裂节点。
另一种方案是长出一个完整的决策树，但随后进行剪枝剪去在开发测试集上不能提高性能的决策节点
预剪枝 后剪枝  限制数的高度
"""
# 6.5 朴素贝叶斯分类器
"""
在朴素贝叶斯分类器中，每个特征都得到发言权，来确定哪个标签应该被分配到一个给
定的输入值。为一个输入值选择标签，朴素贝叶斯分类器以计算每个标签的先验概率开始，
它由在训练集上检查每个标签的频率来确定。之后，每个特征的贡献与它的先验概率组合，
得到每个标签的似然估计。似然估计最高的标签会分配给输入值。
在训练语料中，大多数文档是有关汽车的，所以分类器从接近“汽车”的标签的点上开始。但它会考虑每个特征的
影响。在这个例子中，输入文档中包含的词 dark，它是谋杀之谜的一个不太强的指标，也
包含词 football，它是体育文档的一个有力指标。每个特征都作出了贡献之后，分类器检查
哪个标签最接近，并将该标签分配给输入。
个别特征对整体决策作出自己的贡献，通过“投票反对”那些不经常出现的特征的标签。
特别是，每个标签的似然得分由于与输入值具有此特征的标签的概率相乘而减小。例如：如
果词 run 在 12%的体育文档中出现，在 10%的谋杀之谜的文档中出现，在 2％的汽车文档中
出现，那么体育标签的似然得分将被乘以 0.12，谋杀之谜标签将被乘以 0.1，汽车标签将被
乘以 0.02。整体效果是：略高于体育标签的得分的谋杀之谜标签的得分会减少，而汽车标签
相对于其他两个标签会显著减少
"""
# 潜在概率模型

"""
理解朴素贝叶斯分类器的另一种方式是它为输入选择最有可能的标签，基于下面的假设：
每个输入值是通过首先为那个输入值选择一个类标签，然后产生每个特征的方式产生的，
每个特征与其他特征完全独立。当然，这种假设是不现实的，特征往往高度依赖彼此。我们
将在本节结尾回过来讨论这个假设的一些后果。这简化的假设，称为朴素贝叶斯假设（或独
立性假设），使得它更容易组合不同特征的贡献，因为我们不必担心它们相互影响。基于这个假设，
我们可以计算表达式 P(label|features)，给定一个特别的特征集一个输入具有特定标签的概率。
要为一个新的输入选择标签，我们可以简单地选择使 P(l|features)最大的标签 l。
一开始，我们注意到 P(label|features)等于具有特定标签和特定特征集的输入的概率除以
具有特定特征集的输入的概率：
(2) P(label|features) = P(features, label)/P(features)
接下来，我们注意到 P(features)对每个标签选择都相同。因此，如果我们只是对寻找最
有可能的标签感兴趣，只需计算 P(features,label)，我们称之为该标签的似然
如果我们想生成每个标签的概率估计，而不是只选择最有可能的标签，那么计算 P(features)的最简单的方法是简单的计算
 P(features, label)在所有标签上的总和：
(3) P(features) = Σlabel ∈ labels P(features, label)
标签的似然可以展开为标签的概率乘以给定标签的特征的概率：
(4) P(features, label) = P(label) × P(features|label)
此外，因为特征都是独立的（给定标签），我们可以分离每个独立特征的概率：
(5) P(features, label) = P(label) × ∏f ∈ featuresP(f|label)
这正是我们前面讨论的用于计算标签可能性的方程式：P(label)是一个给定标签的先验
概率，每个 P(f|label)是一个单独的特征对标签可能性的贡献。
"""
# 零计数和平滑
"""
最简单的方法计算 P(f|label)，特征 f 对标签 label 的标签可能性的贡献，是取得具有给
定特征和给定标签的训练实例的百分比：
(6) P(f|label) = count(f, label)/count(label)
然而，当训练集中有特征从来没有和给定标签一起出现时，这种简单的方法会产生一个
问题。在这种情况下，我们的 P(f|label)计算值将是 0，这将导致给定标签的标签可能性为 0。
从而，输入将永远不会被分配给这个标签，不管其他特征有多么适合这个标签。
这里的基本问题与我们计算 P(f|label)有关，对于给定标签输入将具有一个特征的概率。
特别的，仅仅因为我们在训练集中没有看到特征/标签组合出现，并不意味着该组合不会出
现。例如：我们可能不会看到任何谋杀之迷文档中包含词 football，但我们不希望作出结论
认为在这些文档中存在是完全不可能。
虽然 count(f,label)/count(label)当 count(f,label)相对高时是 P(f|label)的好的估计，当 coun
t(f)变小时这个估计变得不那么可靠。因此，建立朴素贝叶斯模型时，我们通常采用更复杂
的技术，被称为平滑技术，用于计算 P(f|label)，给定标签的特征的概率。例如：给定标签的
一个特征的概率的期望似然估计基本上给每个 count(f,label)值加 0.5，Heldout估计使用一个
heldout 语料库计算特征频率与特征概率之间的关系。nltk.probability 模块提供了多种平
滑技术的支持
"""
# 非二元特征
"""
我们这里假设每个特征是二元的，即每个输入要么有这个特征要么没有。标签值特征（例
如：颜色特征，可能有红色、绿色、蓝色、白色或橙色）可通过用二元特征，如“颜色是红
色”，替换它们，将它们转换为二元特征。数字特征可以通过装箱转换为二元特征，装箱是
用特征，如“4<X <6”，替换它们。
另一种方法是使用回归方法模拟数字特征的概率。例如：如果我们假设特征 height 具有
贝尔曲线分布，那么我们可以通过找到每个标签的输入的 height 的均值和方差来估算 P(hei
ght|label)。在这种情况下，P(f=v|label)将不会是一个固定值，会依赖 v 的值变化
"""

# 独立的朴素

"""
朴素贝叶斯分类器被称为“naive（天真、朴素）”的原因是它不切实际地假设所有特征
相互独立（给定标签）。特别的，几乎所有现实世界的问题含有的特征都不同程度的彼此依
赖。如果我们要避免任何依赖其他特征的特征，那将很难构建良好的功能集，提供所需的信
息给机器学习算法。
如果我们忽略了独立性假设，使用特征不独立的朴素贝叶斯分类器会发生什么？产生的
一个问题是分类器“双重计数”高度相关的特征的影响，将分类器推向更接近给定的标签而
不是合理的标签。
来看看这种情况怎么出现的，思考一个包含两个相同的特征 f1 和 f2 的名字性别分类器。
换句话说，f2 是 f1 的精确副本，不包含任何新的信息。当分类器考虑输入，在决定选择哪
一个标签时，它会同时包含 f1 和 f2 的贡献。因此，这两个特征的信息内容将被赋予比它们
应得的更多的比重。
当然，我们通常不会建立包含两个相同的特征的朴素贝叶斯分类器。不过，我们会建立
包含相互依赖特征的分类器。例如：特征 ends-with(a)和 ends-with(vowel)是彼此依赖，
因为如果一个输入值有第一个特征，那么它也必有第二个特征。对于这些功能，重复的信息
可能会被训练集赋予比合理的更多的比重。
"""
# 双重计数的原因
"""
双重计数问题的原因是在训练过程中特征的贡献被分开计算，但当使用分类器为新输入
选择标签时，这些特征的贡献被组合。因此，一个解决方案是考虑在训练中特征的贡献之间
可能的相互作用。然后，我们就可以使用这些相互作用调整独立特征所作出的贡献。
为了使这个更精确，我们可以重写计算标签的可能性的方程，分离出每个功能（或标签）
所作出的贡献
(7) P(features, label) = w[label] × ∏f ∈ features w[f, label]
在这里，w[label]是一个给定标签的“初始分数”，w[f, label]是给定特征对一个标签的
可能性所作的贡献。我们称这些值 w[label]和 w[f, label]为模型的参数或权重。使用朴素贝
叶斯算法，我们单独设置这些参数：
(8) w[label] = P(label)
(9) w[f, label] = P(f|label)
"""

# 6.6 最大熵分类器
"""
最大熵分类器使用了一个与朴素贝叶斯分类器使用的模型非常相似的模型。不是使用概
率设置模型的参数，它使用搜索技术找出一组将最大限度地提高分类器性能的参数。特别的，
它查找使训练语料的整体可能性最大的参数组。其定义如下：
(10) P(features) = Σx ∈ corpus P(label(x)|features(x))
其中 P(label|features)，一个特征为 features 将有类标签 label 的输入的概率，被定义为：
(11) P(label|features) = P(label, features)/Σlabel P(label, features)
由于相关特征的影响之间的潜在的复杂的相互作用，没有办法直接计算最大限度地提高
训练集的可能性的模型参数。因此，最大熵分类器采用迭代优化技术选择模型参数，该技术
用随机值初始化模型的参数，然后反复优化这些参数，使它们更接近最优解。这些迭代优化
技术保证每次参数的优化都会使它们更接近最佳值，但不一定提供方法来确定是否已经达到
最佳值。由于最大熵分类器使用迭代优化技术选择参数，它们花费很长的时间来学习。当训
练集的大小、特征的数目以及标签的数目都很大时尤其如此。
一些迭代优化技术比别的快得多。当训练最大熵模型时，应避免使用广义
迭代缩放（Generalized Iterative Scaling，GIS）或改进的迭代缩放（Improv
ed Iterative Scaling，IIS），这两者都比共轭梯度（Conjugate Gradient，CG）
和 BFGS 优化方法慢很多
"""

# 最大熵模型
"""
最大熵分类器模型是一朴素贝叶斯分类器模型的泛化。像朴素贝叶斯模型一样，最大熵
分类器为给定的输入值计算每个标签的可能性，通过将适合于输入值和标签的参数乘在一
起。朴素贝叶斯分类器模型为每个标签定义一个参数，指定其先验概率，为每个（特征，标
签）对定义一个参数，指定独立的特征对一个标签的可能性的贡献。
相比之下，最大熵分类器模型留给用户来决定什么样的标签和特征组合应该得到自己的
参数。特别的，它可以使用一个单独的参数关联一个特征与一个以上的标签；或者关联一个
以上的特征与一个给定的标签。这有时会允许模型“概括”相关的标签或特征之间的一些差
异。
每个接收它自己的参数的标签和特征的组合被称为一个联合特征。请注意，联合特征是
有标签的的值的属性，而（简单）特征是未加标签的值的属性。
描述和讨论最大熵模型的文字中，术语“特征 features”往往指联合特征；术
语“上下文 contexts”指我们一直说的（简单）特征。
通常情况下，用来构建最大熵模型的联合特征完全镜像朴素贝叶斯模型使用的联合特
征。特别的，每个标签定义的联合特征对应于 w[label]，每个（简单）特征和标签组合定义
的联合特征对应于 w[f, label]。给定一个最大熵模型的联合特征，分配到一个给定输入的标
签的得分与适用于该输入和标签的联合特征相关联的参数的简单的乘积。
(12) P(input, label) = ∏joint-features(input,label)w[joint-feature]
"""

# 熵的最大化

# 假设我们被分配从 10 个可能的任务的列表（标签从 A-J）中为一个给定的词找出正确
# 词意的任务。首先，我们没有被告知其他任何关于词或词意的信息。我们可以为 10 种词意
# 选择的概率分布很多，例如：
#       A   B   C    D   E   F   G   H   I   J
# (i)  10% 10% 10% 10% 10% 10%  10%  10%  10%  10%
# (ii) 5%  15% 0%  30%  0%  8%  12%  0%  6%  24%
# (iii) 0% 100% 0%  0%  0%  0%   0%  0%  0%  0%
"""
虽然这些分布都可能是正确的，我们很可能会选择分布（i），因为没有任何更多的信息，
也没有理由相信任何词的词意比其他的更有可能。另一方面，分布（ii）及（iii）反映的假
设不被我们已知的信息支持。
直觉上这种分布（i）比其他的更“公平”，解释这个的一个方法是引用熵的概念。在决
策树的讨论中，我们描述了熵作为衡量一套标签是如何“无序”。特别的，如果是一个单独
的标签则熵较低，但如果标签的分布比较均匀则熵较高。在我们的例子中，我们选择了分布
（i）因为它的标签概率分布均匀——换句话说，因为它的熵较高。一般情况下，最大熵原
理是说在与我们所知道的一致的的分布中，我们会选择熵最高的
接下来，假设我们被告知词意 A 出现的次数占 55%。再一次，有许多分布与这一条新
信息一致，例如：
      A   B   C  D  E  F  G  H  I  J
(iv) 55% 45% 0% 0% 0% 0% 0% 0% 0% 0%
(v)  55% 5%  5% 5% 5% 5% 5% 5% 5% 5%
(vi) 55% 3% 1% 2% 9% 5% 0% 25% 0% 0%
但是，我们可能会选择最少无根据的假设的分布——在这种情况下，分布（v）。
最后，假设我们被告知词 up 出现在 nearby 上下文中的次数占 10%，当它出现在这个上
下文中时有 80%的可能使用词意 A 或 C。从这个意义上讲，将使用 A 或 C。在这种情况下，
我们很难手工找到合适的分布；然而，可以验证下面的看起来适当的分布：
           A     B     C     D     E     F     G     H    I     J
(vii) +up 5.1%  0.25% 2.9% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25%
      -up 49.9% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46%

    特别的，与我们所知道的一致的分布：如果我们将 A 列的概率加起来是 55％，如果我
们将第 1 行的概率加起来是 10％；如果我们将+up 行词意 A 和 C 的概率加起来是 8%（或+
up 行的 80％）。此外，其余的概率“均匀分布”。
纵观这个例子，我们将自己限制在与我们所知道的一致的分布上。其中，我们选择最高
熵的分布。这正是最大熵分类器所做的。特别的，对于每个联合特征，最大熵模型计算该特
征的“经验频率”——即它出现在训练集中的频率。然后，它搜索熵最大的分布，同时也预
测每个联合特征正确的频率。
"""
#  生成式分类器对比条件式分类器
"""
朴素贝叶斯分类器和最大熵分类器之间的一个重要差异是它们可以被用来回答问题的
类型。朴素贝叶斯分类器是一个生成式分类器的例子，建立一个模型，预测 P(input, label)，
即(input, label)对的联合概率。因此，生成式模型可以用来回答下列问题：
1. 一个给定输入的最可能的标签是什么？
2. 对于一个给定输入，一个给定标签有多大可能性？
3. 最有可能的输入值是什么？
4. 一个给定输入值的可能性有多大？
5. 一个给定输入具有一个给定标签的可能性有多大？
6. 对于一个可能有两个值中的一个值（但我们不知道是哪个）的输入，最可能的标签是什么？
最大熵分类器是条件式分类器的一个例子。条件式分类器建立模型预测 P(label|input)——一个给定输入值的标签的概率。
因此，条件式模型仍然可以被用来回答问题 1 和 2。然而，条件式模型不能用来回答剩下的问题 3-6。一般情况下，
生成式模型确实比条件式模型强大，因为我们可以从联合概率 P(input, label)计算出条件概率 P(label|input)，但反过来不行。
然而，这种额外的能力是要付出代价的。由于该模型更强大的，它也有更多的“自由参数”需要学习的。而训练集的大小是固定的。
因此，使用一个更强大的模型时，我们可用来训练每个参数的值的数据也更少，使其难以找到最佳参数值。
结果是一个生成式模型回答问题 1 和 2 可能不会与条件式模型一样好，因为条件式模型可以集中精力在这两个问题上。
然而，如果我们确实需要像 3-6 问题的答案，那么我们别无选择，只能使用生成式模型。生成式模型与条件式模型之间的差别类似与
一张地形图和一张地平线的图片之间的区别。虽然地形图可用于回答问题的更广泛，制作一张精确的地形图也明显比制作一张精确的
地平线图片更加困难。
"""
# 6.7 为语言模式建模

"""
分类器可以帮助我们理解自然语言中存在的语言模式，允许我们建立明确的模型捕捉这
些模式。通常情况下，这些模型使用有监督的分类技术，但也可以建立分析型激励模型。无
论哪种方式，这些明确的模型有两个重要目的：它们帮助我们了解语言模式，它们可以被用
来预测新的语言数据。
明确的模型可以让我们洞察语言模式，在很大程度上取决于使用哪种模型。一些模型，
如决策树，相对透明，直接给我们信息：哪些因素是决策中重要的，哪些因素是彼此相关的。
另一些模型，如多级神经网络，比较不透明。虽然有可能通过研究它们获得洞察力，但通常
需要大量的工作。
但是，所有明确的模型都可以预测新的未见过的建立模型时未包括在语料中的语言数
据。这些预测进行评估获得模型的准确性。一旦模型被认为足够准确，它就可以被用来自动
预测新的语言数据信息。这些预测模型可以组合成系统，执行很多有用的语言处理任务，例
如：文档分类、自动翻译、问答系统。
理解我们可以从自动构建的模型中学到关于语言的什么是很重要的。处理语言模型时一
个重要的考虑因素是描述性模型与解释性模型之间的区别。描述性模型捕获数据中的模式，
但它们并不提供任何有关数据包含这些模式的原因的信息。例如：我们在表 3.1 中看到的，
同义词 absolutely 和 definitely 是不能互换的：我们说 absolutely adore 而不是 definitely ador
e，definitely prefer 而不是 absolutely prefer。与此相反，解释性模型试图捕捉造成语言模式
的属性和关系。例如：我们可能会介绍一个抽象概念“极性形容词”为一个具有极端意义的
形容词，并对一些形容词进行分类，如：adore 和 detest 是相反的两极。我们的解释性模式
将包含约束：absolutely 只能与极性形容词结合，definitely 只能与非极性形容词结合。总之，
描述性模型提供数据内相关性的信息，而解释性模型再进一步假设因果关系。
大多数从语料库自动构建的模型是描述性模型；换句话说，它们可以告诉我们哪些特征
与一个给定的模式或结构有关，但它们不一定能告诉我们这些特征和模式之间如何关联。如
果我们的目标是理解语言模式，那么我们就可以使用哪些特征是相关的这一信息作为出发
点，设计进一步的实验弄清特征与模式之间的关系。另一方面，如果我们只是对利用该模型
进行预测，例如：作为一种语言处理系统的一部分，感兴趣，那么我们可以使用该模型预测
新的数据，而不用担心潜在的因果关系的细节
 为语料库中的语言数据建模可以帮助我们理解语言模型，也可以用于预测新语言数据。

有监督分类器使用加标签的训练语料库来建立模型，基于输入的特征，预测那个输入的
标签。
有监督分类器可以执行多种 NLP 任务，包括文档分类、词性标注、语句分割、对话行
为类型识别以及确定蕴含关系和很多其他任务。
训练一个有监督分类器时，你应该把语料分为三个数据集：用于构造分类器模型的训练
集，用于帮助选择和调整模型特性的开发测试集，以及用于评估最终模型性能的测试集。
评估一个有监督分类器时，重要的是你要使用新鲜的没有包含在训练集或开发测试集中
的数据。否则，你的评估结果可能会不切实际地乐观。
决策树可以自动地构建树结构的流程图，用于为输入变量值基于它们的特征加标签，虽
然它们易于解释，但不适合处理特性值在决定合适标签过程中相互影响的情况。
在朴素贝叶斯分类器中，每个特征决定应该使用哪个标签的贡献是独立的。它允许特征
值间有关联，但当两个或更多的特征高度相关时将会有问题。
最大熵分类器使用的基本模型与朴素贝叶斯相似；不过，它们使用了迭代优化来寻找使
训练集的概率最大化的特征权值集合。
大多数从语料库自动构建的模型都是描述性的，也就是说，它们让我们知道哪些特征与
给定的模式或结构相关，但它们没有给出关于这些特征和模式之间的因果关系的任何信
息。
"""

