
一、 古腾堡语料库
 import nltk
 nltk.corpus.gutenberg.fileids()   #  加载古腾堡语料库
emma = nltk.corpus.gutenberg.words("austen-emma.txt")   #  获取文章austen-emma.txt
emma = nltk.Text(emma)                        # 对文章进行索引
emma.concordance("surprize")       # 查找
from nltk.corpus import gutenberg  # 导入古腾堡预料
gutenberg.fileids()  # 查看预料

from nltk.book import gutenberg  # 古腾堡预料
for fileid in gutenberg.fileids():
    num_chars = len(gutenberg.raw(fileid))  # 给出原始文本字符数
    num_words = len(gutenberg.words(fileid))  # 告诉我们文本中出现的词汇个数包括词之间的空格
    num_sens = len(gutenberg.sents(fileid))  # 函数把文本划分成句子，其中每一个句子是一个词链表
    num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))
    #      平均词长\平均句长\词语多样性
    print int(num_chars / num_words), int(num_words / num_sens), int(num_words / num_vocab), fileid

二、网络文章和聊天室
from nltk.corpus import webtext  # 网络小文本
from nltk.corpus import nps_chat  # 网络聊天室数据
#  的网络文本小集合的内容包括 Firefox 交流论坛
for fileid in webtext.fileids():
    print fileid, webtext.raw(fileid)[:65], '...'
# 10-19-20s_706posts.xml 包含 2006 年 10 月 19 日从 20 多岁聊天室收集的 706 个帖子
chatroom = nps_chat.posts('10-19-20s_706posts.xml')
print chatroom[123]

三、布朗语料库
A16 ca16 新闻 news Chicago Tribune: Society Reportage
B02 cb02 社论 editorial Christian Science Monitor: Editorials
C17 cc17 评论 reviews Time Magazine: Reviews
D12 cd12 宗教 religion Underwood: Probing the Ethics of Realtors
E36 ce36 爱好 hobbies Norling: Renting a Car in Europe
F25 cf25 传说 lore Boroff: Jewish Teenage Culture
G22 cg22 纯文学 belles_lettres Reiner: Coping with Runaway Technology
H15 ch15 政府 government US Office of Civil and Defence Mobilization: The Fam
ily Fallout Shelter
J17 cj19 博览 learned Mosteller: Probability with Statistical Applications
K04 ck04 小说 fiction W.E.B. Du Bois: Worlds of Color
L13 cl13 推理小说 mystery Hitchens: Footsteps in the Night
M01 cm01 科幻 science_fiction Heinlein: Stranger in a Strange Land
N14 cn15 探险 adventure Field: Rattlesnake Ridge
P12 cp12 言情 romance Callaghan: A Passion in Rome
R06 cr06 幽默 humor Thurber: The Future, If Any, of Comedy
# 引入布朗语料库
from nltk.corpus import brown
brown.categories()  # 布朗语料库的类别
brown.words(categories="news")
brown.words(fileids=['cg22'])
brown.sends(categories=["news","editorial","reviews"])

# 布朗语料库是一个研究文体之间的系统性差异——一种叫做文体学的语言学研究——
# 很方便的资源。让我们来比较不同文体中的情态动词的用法。第一步：产生特定文体的计数。
# 记住做下面的实验之前要
from nltk.corpus import brown
news_text = brown.words(categories='news')
fdist = nltk.FreqDist([w.lower() for w in news_text])
modals = ['can', 'could', 'may', 'might', 'must', 'will']
for m in modals:
    print m + ":",fdist[m]
can: 94 could: 87 may: 93 might: 38 must: 53 will: 389
# 统计情态动词的出现的次数

cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre))
genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
modals = ['can', 'could', 'may', 'might', 'must', 'will']
cfd.tabulate(conditions=genres, samples=modals)
#   结果如下
                    can could may might must will
news                    93 86 66 38 50 389
religion                82 59 78 12 54 71
hobbies                 268 58 131 22 83 264
science_fiction         16 49 4 12 8 16
romance                 74 193 11 51 45 43
humor                   16 30 8 8 9 13

四、路透社语料库
from nltk.corpus import reuters
reuters.fileids()   # 查看训练集和测试集
reuters.categories()  # 查看分类
reuters.categories('training/9865')  # 单个fileids 返回所属分类
reuters.categories(['training/9865', 'training/9880'])  # 多个fileids 返回所属分类
reuters.fileids('barley')   # 查看对应的barley 对应的训练集 和 测试集
reuters.fileids(['barley', 'corn'])   # 查看多个对应的训练集 和 测试集
reuters.words('training/9865')[:14]  # 查看前几个数字 标题一般大写
reuters.words(['training/9865', 'training/9880'])
reuters.words(categories='barley')
reuters.words(categories=['barley','corn'])

五、 就职演说语料库
from nltk.corpus import inaugural
inaugural.fileids()
[fileid[:4] for fileid in inaugural.fileids()] #  截取 0-3  下标的数据
cfd = nltk.ConditionalFreqDist(
        (target, fileid[:4])
        for fileid in inaugural.fileids()
        for w in inaugural.words(fileid)
        for target in ['america', 'citizen']
        if w.lower().startswith(target))
cfd.plot()

六、标注文本语料库
# http://www.nltk.org/data
# http://www.nltk.org/howto
七、在其他语言的语料库
nltk.corpus.cess_esp.words()
nltk.corpus.floresta.words()
nltk.corpus.indian.words('hindi.pos')
nltk.corpus.udhr.fileids()
nltk.corpus.udhr.words('Javanese-Latin1')[11:]

# 累积字长分布：内容是“世界人权宣言”的 6 个翻译版本；此图显示：5 个或 5 个
# 以下字母组成的词在 Ibibio 语言的文本中占约 80％，在德语文本中占 60％，在 Inuktitut 文
# 本中占 25%。
from nltk.corpus import udhr
languages = ['Chickasaw', 'English', 'German_Deutsch','Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
cfd = nltk.ConditionalFreqDist((lang, len(word)) for lang in languages for word in udhr.words(lang + '-Latin1'))
cfd.plot(cumulative = True）

八、文本语料库的结构
NLTK 中定义的基本语料库函数：使用 help(nltk.corpus.reader)可以找到更多的文档，
也可以阅读 http://www.nltk.org/howto 上的在线语料库的 HOWTO
fileids() 语料库中的文件
fileids([categories]) 这些分类对应的语料库中的文件
categories() 语料库中的分类
categories([fileids]) 这些文件对应的语料库中的分类
raw() 语料库的原始内容
raw(fileids=[f1,f2,f3]) 指定文件的原始内容
raw(categories=[c1,c2]) 指定分类的原始内容
words() 整个语料库中的词汇
words(fileids=[f1,f2,f3]) 指定文件中的词汇
words(categories=[c1,c2]) 指定分类中的词汇
sents() 指定分类中的句子
sents(fileids=[f1,f2,f3]) 指定文件中的句子
sents(categories=[c1,c2]) 指定分类中的句子
abspath(fileid) 指定文件在磁盘上的位置
encoding(fileid) 文件的编码（如果知道的话）
open(fileid) 打开指定语料库文件的文件流
root() 到本地安装的语料库根目录的路径

raw = gutenberg.raw("burgess-busterbrown.txt")
raw[1:20]
words = gutenberg.words("burgess-busterbrown.txt")
words[1:20]
sents = gutenberg.sents("burgess-busterbrown.txt")
sents[1:20]

九、载入你自己的语料库
from nltk.corpus import BracketParseCorpusReader
corpus_root = 'D://data//test'
wordlists = PlaintextCorpusReader(corpus_root,'.*')
wordlists.fileids()
wordlists.words('test.txt')
# 匹配合适的文档
corpus_root = r"D:\corpora\penntreebank\parsed\mrg\wsj"
file_pattern = r".*/wsj_.*\.mrg"
ptb = BracketParseCorpusReader(corpus_root, file_pattern)
ptb.fileids()

十、条件频率分布
条件频率分布是频率分布的集合，每个频率分布有一个不同的“条件”。这个条件通常是文本的类别。
    条件和事件：频率分布计算观察到的事件，如文本中出现的词汇。
                条件频率分布需要给每个时间关联一个条件，所以不是处理一个词序列，我们必须处理的是一个配对序列
                每对的形式是：（条件，事件）。如果我们按文体处理整个布朗语料库，将有 15 个条件
                （每个文体一个条件）和 1,161,192 个事件（每一个词一个事件）
十一、按照文本计数词汇
from nltk.corpus import brown
cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre))
genre_word = [(genre,word)] for genre in ['news', 'romance'] for word in brown.words(categories=genre)]
len(genre_word)  # 让我们拆开来看，只看两个文体：新闻和言情。对于每个文体，我们遍历文体中的每个词以产生文体与词的配对
genre_word[:4]  和 genre_word[-4:] 对比
cfd = nltk.ConditionalFreqDist(genre_word)  # 我们可以使用此配对链表创建一个ConditionalFreqDis并将它保存在一个变量cfd
cfd['news']  和  cfd['romance']

十二、绘制分布图和分布表
from nltk.corpus import inaugural
cfd = nltk.ConditionalFreqDist((target, fileid[:4])
 for fileid in inaugural.fileids()
 for w in inaugural.words(fileid)
 for target in ['america', 'citizen']
 if w.lower().startswith(target))
 cfd.plot()
 # 这次的条件是语言的名称，图中的计数来源于词长。它利用了每一种语言的文件名是语言名称后面跟'-Latin1'（字符编码）的事实
from nltk.corpus import udhr
languages = ['Chickasaw', 'English', 'German_Deutsch','Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
cfd = nltk.ConditionalFreqDist(
(lang, len(word))
for lang in languages
for word in udhr.words(lang + '-Latin1'))
cf.plot()  # 在 plot()和 tabulate()方法中，我们可以使用 conditions= parameter 来选择指定 哪些条件显示。
#  如果我们忽略它，所有条件都会显示我们可以使用 samples= parameter 来限制要显示的样本。

"""
我们可以为两种语言和长度少于 10 个字符的词汇绘制累计频率数据表，如下所示。
我们解释一下上排最后一个单元格中数值的含义是英文文本中9个或少于9个字符长的词有
1,638 个
"""
cfd.tabulate(conditions=['English', 'German_Deutsch'],samples=range(10), cumulative=True)

十三、使用双连词生成随机文本
sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']
list(nltk.bigrams(sent))  #  生成随机词组
"""
产生随机文本：此程序获得《创世记》文本中所有的双连词，然后构造一个条件频率分
布来记录哪些词汇最有可能跟在给定词的后面；例如：living 后面最可能的词是 creature；gene
rate_model()函数使用这些数据和种子词随机产生文本
"""
def generate_model(cfdist, word, num=15):
    for i in range(num):
        print word,
        word = cfdist[word].max()
text = nltk.corpus.genesis.words('english-kjv.txt')
bigrams = nltk.bigrams(text)
cfd = nltk.ConditionalFreqDist(bigrams)
generate_model(cfd, 'living')

cfdist= ConditionalFreqDist(pairs) 从配对链表中创建条件频率分布
cfdist.conditions() 将条件按字母排序
cfdist[condition] 此条件下的频率分布
cfdist[condition][sample] 此条件下给定样本的频率
cfdist.tabulate() 为条件频率分布制表
cfdist.tabulate(samples, conditions) 指定样本和条件限制下制表
cfdist.plot() 为条件频率分布绘图
cfdist.plot(samples, conditions) 指定样本和条件限制下绘图
cfdist1 < cfdist2 测试样本在 cfdist1 中出现次数是否小于在 cfdist2 中出现次数

#  转化为复数形式
def plural(word):
    if word.endswith('y'):
        return word[:-1] + 'ies'
    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
        return word + 'es'
    elif word.endswith('an'):
        return word[:-2] + 'en'
    else:
        return word + 's

# 过滤文本：此程序计算文本的词汇表，然后删除所有在现有的词汇列表中出现的元素，只留下罕见或拼写错误的词
def unusual_words(text):
    text_vocab = set(w.lower() for w in text if w.isalpha())
    english_vocab = set(w.lower() for w in nltk.corpus.words.words())
    unusual = text_vocab.difference(english_vocab)
    return sorted(unusual)
unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))
# 停用词
from nltk.corpusimport stopwords
# 计算文本中停用词的占比
def content_fraction(text):
    stopwords = nltk.corpus.stopwords.words('english')
    content = [w for w in text if w.lower() not in stopwords]
    return len(content) / len(text)

content_fraction(nltk.corpus.reuters.words())
"""
一个字母拼词谜题：在由随机选择的字母组成的网格中，选择里面的字母组成词。
这个谜题叫做“目标”。图中文字的意思是：用这里显示的字母你能组成多少个 4 字母或者
更多字母的词？每个字母在每个词中只能被用一次。每个词必须包括中间的字母并且必须至
少有一个 9 字母的词。没有复数以“s”结尾；没有外来词；没有姓名。能组出 21 个词就是
“好”；32 个词，“很好”；42 个词，“非常好”
"""
puzzle_letters = nltk.FreqDist('egivrvonl')
obligatory = 'r'
wordlist = nltk.corpus.words.words()
[w for w in wordlist
    if len(w) >= 6    # 限制词长
        and obligatory in w   # 限制词中必须出现
        # FreqDist 比较法允许我们检查每个字母在候选词中的频率是否小于或等于相应的字母在拼词谜题中的频率
        and nltk.FreqDist(w) <= puzzle_letters]
"""
另一个词汇列表是名字语料库，包括 8000 个按性别分类的名字。男性和女性的名字存
储在单独的文件中。让我们找出同时出现在两个文件中的名字即性别暧昧的名字：
"""
names = nltk.corpus.names
names.fileids()
['female.txt', 'male.txt']
male_names = names.words('male.txt')
female_names = names.words('female.txt')
[w for w in male_names if w in female_names]

# 男性与女性姓名中 的分布
cfd = nltk.ConditionalFreqDist(
 (fileid, name[-1])
 for fileid in names.fileids()
 for name in names.words(fileid))
 cfd.plot()

十五、发音的词典
# 在每一行中含有一个词加一些性质。
entries = nltk.corpus.cmudict.entries()
len(entries)
for entry in entries[39943:39951]:
     print entry

十六、比较词表
# 表格词典的另一个例子是比较词表。NLTK 中包含了所谓的斯瓦迪士核心词列表（Swa
# desh wordlists），几种语言中约 200 个常用词的列表。语言标识符使用 ISO639 双字母码
from nltk.corpus import swadesh
swadesh.fileids()
swadesh.words('en')
# 我们可以通过在 entries()方法中指定一个语言链表来访问多语言中的同源词。更进一步，我们可以把它转换成一个简单的词典
fr2en = swadesh.entries(['fr', 'en'])
translate = dict(fr2en)
translate['chien']

de2en = swadesh.entries(['de', 'en']) # German-English
es2en = swadesh.entries(['es', 'en']) # Spanish-English
translate.update(dict(de2en))
translate.update(dict(es2en))
translate['Hund']
'dog'
translate['perro']
'dog'
# 我们可以比较日尔曼语族和拉丁语族的不同
languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']
for i in [139, 140, 141, 142]:
    print swadesh.entries(languages)[i]

十七、词汇工具：Toolbox和Shoebox
可能最流行的语言学家用来管理数据的工具是 Toolbox（工具箱），以前叫做 Shoebox
（鞋柜），因为它用满满的档案卡片占据了语言学家的旧鞋盒。Toolbox 可以免费从 http://www.sil.org/computing/toolbox/下载。
一个 Toolbox 文件由一个大量条目的集合组成，其中每个条目由一个或多个字段组成。
大多数字段都是可选的或重复的，这意味着这个词汇资源不能作为一个表格或电子表格来处理。
下面是一个罗托卡特语（Rotokas）的词典。我们只看第一个条目，词 kaa 的意思是 togag：
from nltk.corpus import toolbox
toolbox.entries('rotokas.dic')

十八、 WordNet
    意义与同义词
   from nltk.corpusimport wordnet as wn
   wn.synsets('motorcar')
   [Synset('car.n.01')]
因此，motorcar 只有一个可能的含义，它被定义为 car.n.01，car 的第一个名词意义。
car.n.01 被称为 synset 或“同义词集”，意义相同的词（或“词条”）的集合：
wn.synset('car.n.01').lemma_names
['car', 'auto', 'automobile', 'machine', 'motorcar']
同义词集中的每个词可以有多种含义，例如：car 也可能是火车车厢、一个货车或电梯
厢。但我们只对这个同义词集中所有词来说最常用的一个意义感兴趣

wn.synset('car.n.01').definition
  'a motor vehicle with four wheels; usually propelled by an internal combustion engine'
wn.synset('car.n.01').examples
   ['he needs a car to get to work']

 虽然定义帮助人们了解一个同义词集的本意，同义词集中的词往往对我们的程序更有
用。为了消除歧义，我们将这些词标注为 car.n.01.automobile，car.n.01.motorcar 等。
这种同义词集和词的配对叫做词条。
wn.synset('car.n.01').lemmas  # 我们可以得到指定同义词集的所有词条
wn.lemma('car.n.01.automobile')   # 查找特定的词条
wn.lemma('car.n.01.automobile').synset  #  得到一个词条对应的同义词集
wn.lemma('car.n.01.automobile').name # 可以得到一个词条的“名字”
wn.synsets('car')   #词 car 是含糊的，有五个同义词集
for synset in wn.synsets('car'):
     print synset.lemma_names
wn.lemmas('car')  # 访问所有包含词 car 的词条

十九、WordNet 的层次结构
WordNet 的同义词集对应于抽象的概念，它们并不总是有对应的英语词汇。这些概念在
层次结构中相互联系在一起。一些概念也很一般，如实体、状态、事件；这些被称为独一无
二的根同义词集。其他的，如：油老虎和有仓门式后背的汽车等就比较具体的多
一个如摩托车这样的概念，我们可以看到它的更加具体（直接）的概念——下位词。
motorcar = wn.synset('car.n.01')
types_of_motorcar = motorcar.hyponyms()
sorted([lemma.name for synset in types_of_motorcar for lemma in synset.lemmas])
我们也可以通过访问上位词来浏览层次结构。有些词有多条路径，因为它们可以归类在一个以上的分类中。
car.n.01 与 entity.n.01 之间有两条路径，因为 wheeled_vehicle.n.
01 可以同时被归类为车辆和容器
motorcar.hypernyms()
paths = motorcar.hypernym_paths()
[synset.name for synset in paths[0]]
[synset.name for synset in paths[1]]
motorcar.root_hypernyms()
上位词和下位词被称为词汇关系，因为它们是同义集之间的关系。这个关系定位上下为
“是一个”层次。WordNet 网络另一个重要的漫游方式是从物品到它们的部件（部分）或到
它们被包含其中的东西（整体）。例如：一棵树的部分是它的树干，树冠等；这些都是 par
t_meronyms()。一棵树的实质是包括心材和边材组成的，即 substance_meronyms()。
树木的集合形成了一个森林，即 member_holonyms()：
wn.synset('tree.n.01').part_meronyms()
wn.synset('tree.n.01').substance_meronyms()
wn.synset('tree.n.01').member_holonyms()
for synset in wn.synsets('mint', wn.NOUN):
    print synset.name + ':', synset.definition
wn.synset('mint.n.04').part_holonyms()
wn.synset('mint.n.04').substance_holonyms()

  语义相似度
如果两个同义词集共用一个非常具体的上位词——在上位词层次结构中处于较低层的上位词——它们一定有密切的联系。
right = wn.synset('right_whale.n.01')
orca = wn.synset('orca.n.01')
minke = wn.synset('minke_whale.n.01')
tortoise = wn.synset('tortoise.n.01')
novel = wn.synset('novel.n.01')
right.lowest_common_hypernyms(minke)
right.lowest_common_hypernyms(orca)
right.lowest_common_hypernyms(tortoise)
right.lowest_common_hypernyms(novel)
wn.synset('baleen_whale.n.01').min_depth()
wn.synset('whale.n.02').min_depth()
wn.synset('vertebrate.n.01').min_depth()
wn.synset('entity.n.01').min_depth()
WordNet 同义词集的集合上定义了类似的函数能够深入的观察。例如：path_similari
tyassigns 是基于上位词层次结构中相互连接的概念之间的最短路径在 0-1 范围的打分（两
者之间没有路径就返回-1）。同义词集与自身比较将返回 1。考虑以下的相似度：露脊鲸与
小须鲸、逆戟鲸、乌龟以及小说。数字本身的意义并不大，当我们从海洋生物的语义空间转
移到非生物时它是减少的。
right.path_similarity(minke)
right.path_similarity(orca)
right.path_similarity(tortoise)
right.path_similarity(novel)
还有一些其它的相似性度量方法；你可以输入 help(wn)获得更多信息。NLT
K 还包括 VerbNet，一个连接到 WordNet 的动词的层次结构的词典。
文本语料库是一个大型结构化文本的集合。NLTK 包含了许多语料库，如：布朗语料库
nltk.corpus.brown。
有些文本语料库是分类的，例如通过文体或者主题分类；有时候语料库的分类会相互重叠。
条件频率分布是一个频率分布的集合，每个分布都有一个不同的条件。它们可以用于通
过给定内容或者文体对词的频率计数。
行数较多的 Python 程序应该使用文本编辑器来输入，保存为.py 后缀的文件，并使用 import 语句来访问。
Python 函数允许你将一段特定的代码块与一个名字联系起来，然后重用这些代码想用
多少次就用多少次。
一些被称为“方法”的函数与一个对象联系在起来，我们使用对象名称跟一个点然后跟
方法名称来调用它，就像：x.funct(y)或者 word.isalpha()。
要想找到一些关于变量 v 的信息，可以在 Pyhon 交互式解释器中输入 help(v)来阅读这
一类对象的帮助条目。WordNet 是一个面向语义的英语词典，由同义词的集合—或称为同义词集（synsets）—
组成，并且组织成一个网络。默认情况下有些函数是不能使用的，必须使用 Python 的 import 语句来访问。
