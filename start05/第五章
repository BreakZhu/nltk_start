将词汇按它们的词性（parts-of-speech，POS）分类以及相应的标注它们的过程被称为词
性标注（part-of-speech tagging, POS tagging）或干脆简称标注。词性也称为词类或词汇范
畴。用于特定任务的标记的集合被称为一个标记集。我们在本章的重点是利用标记和自动标注文本
一个词性标注器（part-of-speech tagger 或 POS tagger）处理一个词序列，为每个词附
加一个词性标记（不要忘记 import nltk）：
text = nltk.word_tokenize("And now for something completely different")
nltk.pos_tag(text)
[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),
('completely', 'RB'), ('different', 'JJ')]
在这里我们看到 and 是 CC，并列连词；now 和 completely 是 RB，副词；for 是 IN，介
词；something 是 NN，名词；different 是 JJ，形容词。
text = nltk.word_tokenize("They refuse to permit us to obtain the refuse permit")
nltk.pos_tag(text)
[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'),
('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]
ADJ 形容词 new, good, high, special, big, local
ADV 动词 really, already, still, early, now
CNJ 连词 and, or, but, if, while, although
DET 限定词 the, a, some, most, every, no
EX 存在量词 there, there's
FW 外来词 dolce, ersatz, esprit, quo, maitre
MOD 情态动词 will, can, would, may, must, should
N 名词 year, home, costs, time, education
NP 专有名词 Alison, Africa, April, Washington
NUM 数词 twenty-four, fourth, 1991, 14:24
PRO 代词 he, their, her, its, my, I, us
P 介词 on, of, at, with, by, into, under
TO 词 to to
UH 感叹词 ah, bang, ha, whee, hmpf, oops
V 动词 is, has, get, do, make, see, run
VD 过去式 said, took, told, made, asked
VG 现在分词 making, going, playing, working
VN 过去分词 given, taken, begun, sung
WH Wh 限定词 who, which, when, what, where, how
from nltk.corpus import brown
brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True)
tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)
tag_fd.keys()
tag_fd.plot(20, cumulative=True)  # 打印前20 分布

找出最频繁的名词标记的程序
def findtags(tag_prefix, tagged_text):
    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text if tag.startswith(tag_prefix))
    return dict((tag, cfd[tag].keys()[:5]) for tag in cfd.conditions())
tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news'))
for tag in sorted(tagdict):
    print tag, tagdict[tag]

# 我们考虑句子中的每个三词窗口，检查它们是否符合我们的标准。如果标记匹配，我们输出对应的词
def process(sentence):
    for (w1, t1), (w2, t2), (w3, t3) in nltk.trigrams(sentence):  # nltk trigrams 三元分词窗口
        if t1.startswith('V') and t2 == 'TO' and t3.startswith('V'):
            print w1, w2, w3
for tagged_sent in brown.tagged_sents():
    process(tagged_sent)

# 最后，让我们看看与它们的标记关系高度模糊不清的词。
# 了解为什么要标注这样的词是因为它们各自的上下文可以帮助我们弄清楚标记之间的区别
brown_news_tagged = brown.tagged_words(categories='news')
data = nltk.ConditionalFreqDist((word.lower(), tag) for (word, tag) in brown_news_tagged)
for word in data.conditions():
    if len(data[word]) > 3:
        tags = data[word].keys()
        print word, ' '.join(tags)

使用 Python 字典映射词及其属性 字典映射词及其属性 字典映射词及其属性
pos={'furiously': 'ADV', 'ideas': 'N', 'colorless': 'ADJ', 'sleep': 'V'}
pos = dict(colorless='ADJ', ideas='N', sleep='V', furiously='ADV')
list(pos)
sorted(pos)
[w for w in pos if w.endswith('s')]
nltk 默认字典
frequency = nltk.defaultdict(int)
frequency['colorless'] = 4
frequency['ideas']
pos = nltk.defaultdict(list)
pos['sleep'] = ['N', 'V']
pos['ideas']

创建一个任一条目的默认值是'N'的字典。当我们访问一个不存在
的条目时，它会自动添加到字典。
pos = nltk.defaultdict(lambda: 'N')
pos['colorless'] = 'ADJ'
pos['blog']
pos.items()
我们可以预处理一个文本，在一个默认字典的帮助下，替换低频词汇为一个特殊的“超出词汇表”标识符，UNK（out of vocabulary)
我们需要创建一个默认字典，映射每个词为它们的替换词。最频繁的 n 个词将被映射到它们自己。其他的被映射到 UNK。

alice = nltk.corpus.gutenberg.words('carroll-alice.txt')
vocab = nltk.FreqDist(alice)
v1000 = list(vocab)[:1000]
mapping = nltk.defaultdict(lambda: 'UNK')
for v in v1000:
    mapping[v] = v
alice2 = [mapping[v] for v in alice]
alice2[:100]
# 递增地更新字典，按值降序排序
counts = nltk.defaultdict(int)
from nltk.corpus import brown

for (word, tag) in brown.tagged_words(categories='news'):
    counts[tag] += 1
print counts['N']
print list(counts)
from operator import itemgetter

# 第二个参数使用函数 itemgetter()指定排序键。在一般情况下，itemgetter(n)
# 返回一个函数，这个函数可以在一些其他序列对象上被调用获得这个序列的第 n 个元素
# 的最后一个参数指定项目是否应被按相反的顺序返回，即频率值递减。
print sorted(counts.items(), key=itemgetter(1), reverse=True)
# 第二种处理方式 按它们最后两个字母索引词汇
last_letters = nltk.defaultdict(list)
words = nltk.corpus.words.words('en')
for word in words:
    key = word[-2:]
    last_letters[key].append(word)
print last_letters['ly']
# 下面的例子使用相同的模式创建一个颠倒顺序的词字典
anagrams = nltk.defaultdict(list)
for word in words:
    key = ''.join(sorted(word))
    anagrams[key].append(word)
print anagrams['aeilnrt']
# 由于积累这样的词是如此常用的任务，NLTK 以 nltk.Index()的形式提供一个创建 defaultdict(list)更方便的方式
anagrams = nltk.Index((''.join(sorted(w)), w) for w in words)
print anagrams['aeilnrt']

# 它的条目的默认值也是一个字典（其默认值是 int()，即 0）。请注意我们如何遍历已标注语料库的双连词，
# 每次遍历处理一个词-标记对每次通过循环时，我们更新字典 pos 中的条目 (t1, w2)，一个标记和它后面的词。当我们在 pos
# 中查找一个项目时，我们必须指定一个复合键�，然后得到一个字典对象。一个 POS 标注
# 器可以使用这些信息来决定词 right，前面是一个限定词时，应标注为 ADJ。
pos = nltk.defaultdict(lambda: nltk.defaultdict(int))
brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')
for ((w1, t1), (w2, t2)) in nltk.bigrams(brown_news_tagged):
    pos[(t1, w2)][t2] += 1
print pos[('DET', 'right')]

颠倒字典
字典支持高效查找，只要你想获得任意键的值。如果 d 是一个字典，k 是一个键，输入
d[K]，就立即获得值。给定一个值查找对应的键要慢一些和麻烦一些：
counts = nltk.defaultdict(int)
for word in nltk.corpus.gutenberg.words('milton-paradise.txt'):
    counts[word] += 1
[key for (key, value) in counts.items() if value == 32]

# 为了防止key 出现重复 需要使用list
pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}
pos2 = dict((value, key) for (key, value) in pos.items())
pos.update({'cats': 'N', 'scratch': 'V', 'peacefully': 'ADV', 'old': 'ADJ'})
pos2 = nltk.defaultdict(list)
for key, value in pos.items():
    pos2[value].append(key)

常用的方法与字典相关习惯用法的总结
d = {} 创建一个空的字典，并将分配给 d
d[key] = value 分配一个值给一个给定的字典键
d.keys() 字典的键的链表
list(d) 字典的键的链表
sorted(d) 字典的键，排序
key in d 测试一个特定的键是否在字典中
for key in d 遍历字典的键
d.values() 字典中的值的链表
dict([(k1,v1), (k2,v2), ...]) 从一个键-值对链表创建一个字典
d1.update(d2) 添加 d2 中所有项目到 d1
defaultdict(int) 一个默认值为 0 的字典
自动标注
from nltk.corpus import brown
brown_tagged_sents = brown.tagged_sents(categories='news')
brown_sents = brown.sents(categories='news')
tags = [tag for (word, tag) in brown.tagged_words(categories='news')]
nltk.FreqDist(tags).max()
# 现在我们可以创建一个将所有词都标注成 NN 的标注器
raw = 'I do not like green eggs and ham, I do not like them Sam I am!'
tokens = nltk.word_tokenize(raw)
default_tagger = nltk.DefaultTagger('NN')
print default_tagger.tag(tokens)
print default_tagger.evaluate(brown_tagged_sents)

# 正则表达式标注器

"""
我们可能会猜测任一以 ed
结尾的词都是动词过去分词，任一以's 结尾的词都是名词所有格。可以用一个正则表达式的
列表表示这些
"""
patterns = [(r'.*ing$', 'VBG'),  # gerunds
            (r'.*ed$', 'VBD'),  # simple past
            (r'.*es$', 'VBZ'),  # 3rd singular present
            (r'.*ould$', 'MD'),  # modals
            (r'.*\'s$', 'NN$'),  # possessive nouns
            (r'.*s$', 'NNS'),  # plural nouns
            (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers
            (r'.*', 'NN')  # nouns (default)
            ]
regexp_tagger = nltk.RegexpTagger(patterns)
regexp_tagger.tag(brown_sents[3])
print regexp_tagger.evaluate(brown_tagged_sents)

# 查询标注器
"""
    很多高频词没有 NN 标记。让我们找出 100 个最频繁的词，存储它们最有可能的标记。
    然后我们可以使用这个信息作为“查找标注器”（NLTK UnigramTagger）的模型
"""
fd = nltk.FreqDist(brown.words(categories='news'))
cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))
most_freq_words = fd.keys()[:1000]
likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)
baseline_tagger = nltk.UnigramTagger(model=likely_tags)
print baseline_tagger.evaluate(brown_tagged_sents)
sent = brown.sents(categories='news')[3]
print baseline_tagger.tag(sent)


# 查找标注器的性能，使用不同大小的模型。
def performance(cfd, wordlist):
    lt = dict((word, cfd[word].max()) for word in wordlist)
    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))
    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))


def display():
    import pylab
    words_by_freq = list(nltk.FreqDist(brown.words(categories='news')))
    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))
    sizes = 2 ** pylab.arange(16)
    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]
    pylab.plot(sizes, perfs, '-bo')
    pylab.title('Lookup Tagger Performance with Varying Model Size')
    pylab.xlabel('Model Size')
    pylab.ylabel('Performance')
    pylab.show()
"""
随着模型规模的增长，最初的性能增加迅速，最终达到一个稳定水平，这
时模型的规模大量增加性能的提高很小
"""
display()

# 评估
# 这些工具的性能评估是 NLP 的一个中心主题

#  N-gram 标注
# 一元标注（Unigram Unigram Unigram Unigram Tagging Tagging Tagging Tagging）
"""
一元标注器基于一个简单的统计算法：对每个标识符分配这个独特的标识符最有可能的
标记。例如：它将分配标记 JJ 给词 frequent 的所有出现，因为 frequent 用作一个形容词（例
如：a frequent word）比用作一个动词（例如：I frequent this cafe）更常见。一个一元标
注器的行为就像一个查找标注器（5.4 节），除了有一个更方便的建立它的技术，称为训练。
在下面的代码例子中，我们训练一个一元标注器，用它来标注一个句子，然后评估：
我们训练一个 UnigramTagger，通过在我们初始化标注器时指定已标注的句子数据作
为参数。训练过程中涉及检查每个词的标记，将所有词的最可能的标记存储在一个字典里面
这个字典存储在标注器内部
"""
from nltk.corpus import brown

brown_tagged_sents = brown.tagged_sents(categories='news')
brown_sents = brown.sents(categories='news')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.tag(brown_sents[2007])
print unigram_tagger.evaluate(brown_tagged_sents)

# 分离训练和测试数据
"""
在前面的例子,一个只是记忆它的训练数据，而不试图建立一个一般的模型的标
注器会得到一个完美的得分，但在标注新的文本时将是无用的。
相反，我们应该分割数据，90％为测试数据，其余 10％为测试数据
，90％为测试数据，其余 10％为测试数据
"""
size = int(len(brown_tagged_sents) * 0.9)
train_sents = brown_tagged_sents[:size]
test_sents = brown_tagged_sents[size:]
unigram_tagger = nltk.UnigramTagger(train_sents)
print unigram_tagger.evaluate(test_sents)

# 一般的 N-gram 的标注
"""
一个 n-gram 标注器是一个 unigram 标注器的一般化，它的上下文是当前词和它前面 n1
个标识符的词性标记
1-gram标注器是一元标注器（unigram tagger）另一个名称：即用于标注
一个标识符的上下文的只是标识符本身。2-gram 标注器也称为二元标注器
（bigram taggers），3-gram 标注器也称为三元标注器（trigram taggers）
NgramTagger 类使用一个已标注的训练语料库来确定对每个上下文哪个词性标记最
有可能。在这里，我们看到一个 n-gram 标注器的特殊情况，即一个 bigram 标注器。首先，
我们训练它，然后用它来标注未标注的句子
"""
bigram_tagger = nltk.BigramTagger(train_sents)
print bigram_tagger.tag(brown_sents[2007])
unseen_sent = brown_sents[4203]
print bigram_tagger.tag(unseen_sent)
print bigram_tagger.evaluate(test_sents)

# 组合标注器
"""
    解决精度和覆盖范围之间的权衡的一个办法是尽可能的使用更精确的算法，但却在很多
    时候落后于具有更广覆盖范围的算法。例如：我们可以按如下方式组合 bigram 标注器、uni
    gram 标注器和一个默认标注器：
    1. 尝试使用 bigram 标注器标注标识符。
    2. 如果 bigram 标注器无法找到一个标记，尝试 unigram 标注器。
    3. 如果 unigram 标注器也无法找到一个标记，使用默认标注器
    将会丢弃那些只看到一次或两次的上下文
    nltk.BigramTagger(sents, cutoff=2, backoff=t1)
"""
t0 = nltk.DefaultTagger('NN')
t1 = nltk.UnigramTagger(train_sents, backoff=t0)
t2 = nltk.BigramTagger(train_sents, backoff=t1)
print t2.evaluate(test_sents)

# 标注生词
"""
我们标注生词的方法仍然是回退到一个正则表达式标注器或一个默认标注器。这些都无
法利用上下文。因此，如果我们的标注器遇到词 blog，训练过程中没有看到过，它会分配相
同的标记，不论这个词出现的上下文是 the blog 还是 to blog。我们怎样才能更好地处理这
些生词，或词汇表以外的项目？
一个有用的基于上下文标注生词的方法是限制一个标注器的词汇表为最频繁的 n 个词，
使用 5.3 节中的方法替代每个其他的词为一个特殊的词 UNK。训练时，一个 unigram 标注器
可能会学到 UNK 通常是一个名词。然而，n-gram 标注器会检测它的一些其他标记中的上下
文。例如：如果前面的词是 to（标注为 TO），那么 UNK 可能会被标注为一个动词
"""
# 存储标注器

"""
在大语料库上训练一个标注器可能需要大量的时间。没有必要在每次我们需要的时候训
练一个标注器，很容易将一个训练好的标注器保存到一个文件以后重复使用。
"""
from cPickle import dump

output = open('t2.pkl', 'wb')
dump(t2, output, -1)
output.close()
# 现在，我们可以在一个单独的 Python 进程中载入我们保存的标注器：
from cPickle import load

input = open('t2.pkl', 'rb')
tagger = load(input)
input.close()
text = """The board's action shows what free enterprise is up against in our complex maze of regulatory laws ."""
tokens = text.split()
print tagger.tag(tokens)

# 性能限制
# 考虑一个trigram它遇到多少词性歧义的情况 我们可以根据经验决定这个问题的答案
"""
1/20 的 trigrams 是有歧义的 。给定当前单词及其前两个标记，根据训练数据，在
5％的情况中，有一个以上的标记可能合理地分配给当前词。假设我们总是挑选在这种含糊
不清的上下文中最有可能的标记，可以得出 trigram 标注器性能的一个下界
"""
cfd = nltk.ConditionalFreqDist(((x[1], y[1], z[0]), z[1]) for sent in brown_tagged_sents for x, y, z in nltk.trigrams(sent))
ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]
print sum(cfd[c].N() for c in ambiguous_contexts)*1.0 / cfd.N()

"""
调查标注器性能的另一种方法是研究它的错误。有些标记可能会比别的更难分配，可能
需要专门对这些数据进行预处理或后处理。一个方便的方式查看标注错误是混淆矩阵。它用
图表表示期望的标记（黄金标准）与实际由标注器产生的标记
"""
test_tags = [tag for sent in brown.sents(categories='editorial')
for (word, tag) in t2.tag(sent)]
gold_tags = [tag for (word, tag) in brown.tagged_words(categories='editorial')]
print nltk.ConfusionMatrix(gold_tags, test_tags)

# 跨句子边界标注
# 句子层面的 N-gram 标注
brown_tagged_sents = brown.tagged_sents(categories='news')
brown_sents = brown.sents(categories='news')
size = int(len(brown_tagged_sents) * 0.9)
train_sents = brown_tagged_sents[:size]
test_sents = brown_tagged_sents[size:]
t0 = nltk.DefaultTagger('NN')
t1 = nltk.UnigramTagger(train_sents, backoff=t0)
t2 = nltk.BigramTagger(train_sents, backoff=t1)
print t2.evaluate(test_sents)
"""
n-gram 标注器从前面的上下文中获得的唯一的信息是标
记，虽然词本身可能是一个有用的信息源。n-gram 模型使用上下文中的词的其他特征为条
件是不切实际的。在本节中，我们考察 Brill 标注，一种归纳标注方法，它的性能很好，使
用的模型只有 n-gram 标注器的很小一部分
"""
print nltk.tag.brill.nltkdemo18()

如何确定一个词的分类
206